# -*- coding: utf-8 -*-
"""Cybersecurity_EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18469mFY9KjplKiMI1k4LMvLM0JpWBuk8

# <p><img alt="Colaboratory logo" height="45px" src="/img/colab_favicon.ico" align="left" hspace="10px" vspace="0px"></p>What is Colaboratory?

Colaboratory, or "Colab" for short, allows you to write and execute Python in your browser, with 
- Zero configuration required
- Free access to GPUs
- Easy sharing

Whether you're a **student**, a **data scientist** or an **AI researcher**, Colab can make your work easier. Watch [Introduction to Colab](https://www.youtube.com/watch?v=inN8seMm7UI) to learn more, or just get started below!

# Introduction

<h1>Exploratory Data Analysis (EDA) Walkthrough</h1>

The goal of this walkthrough is to help show you the process done by analysts and researchers. EDA is one of the most important first steps when dealing with new data. 

<i>EDA allows us to better understand the data and the tools we can use in future steps.</i>

<h1> Business Context</h1>

You are a new cybersecurity analyst, and have been tasked with finding out some more information on recent events that have happened over our network.

-----

1. Most targeted Destination IP Address
2. Most Ports attacked
3. Most Frequently/common type of Attack
4. Different time of the day , (odd , hours, day or night)
5. Find any patterns?

-----

## Importing Libraries

The following lines of code are installing certain libraries or dependencies for the notebook to run properly.
"""

import pandas as pd
import seaborn as sns
import glob
import matplotlib.pyplot as plt
import ipaddress
import numpy as np
from scipy import stats
from scipy.stats import chi2_contingency
from datetime import datetime, timedelta
import math
import missingno as msno
plt.style.use('ggplot')
import warnings
warnings.filterwarnings('ignore')

"""## Load Data"""

df1 = pd.read_csv('https://media.githubusercontent.com/media/BradleyConlin/EDA_Tutorials/main/Network_Traffic_Events_1.csv')

df1.shape

df2 = pd.read_csv('https://media.githubusercontent.com/media/BradleyConlin/EDA_Tutorials/main/Network_Traffic_Events_2.csv')

df2.shape

df3 = pd.read_csv('https://media.githubusercontent.com/media/BradleyConlin/EDA_Tutorials/main/Network_Traffic_Events_3.csv')

df3.shape

combined = [df1, df2, df3]
df = pd.concat(combined)
df.shape

"""# Step 1: Data Exploration

## Initial Discovery

We will now begin to gain insight on the data with the following commands:

- **df.shape**    (row, column)
- **df.columns**  (column titles)
- **df.head**     (top 5 results in table format)
- **df.nunique**  (count of unique values for each variable/dimension)
"""

df.shape

"""This shows us that there are 11 columns of data, and there are 146,615 rows of data."""

df.columns

df.head()

"""**df.head()** returns the top 5 results of the data in table format. By changing the value in "()", you can change how many rows you can view. """

df.head(2)

df.nunique(axis=0)

"""**df.nunique** will show help us understand the type and spread of the data. Most notable would be the "." variable, with only has a single unique value.

# Step 2: Data Cleaning

## Initial Cleaning

Data cleaning is used to not only increase the data integrity, but to also make it more usable and understandable to humans.

We will begin with the time variable. It is currently in a 10-digit serial format, we need to convert it to normal data time.

___

<i>***Example of Feature Engineering***</i>
"""

df[['Start time','Last time']] = df['Time'].str.split('-',expand=True)
df.head(2)

df['Start time']

"""Now that we know the format, and the beginning and end times in the data, we will transform the data, and add to it a duration column in seconds."""

df['Start time'] = pd.to_datetime(df['Start time'], unit='s')
df['Last time'] = pd.to_datetime(df['Last time'], unit='s')
df['Duration'] = ((df['Last time'] - df['Start time']).dt.seconds).astype(int)

df.head()

"""Our df looks much better, but we do not need the original time format anymore. So we will use df.drop to remove this df, and then check to make sure we did it correctly."""

df = df.drop(['Time'],axis=1)

df.head(1)

df['Start time'].astype(str).str.split(' ').str[0].unique()

"""As you can see from the above table, the start and Last time are now given in YYYY-MM-DD Time of Day.

For ease and time, we will be looking at two days in partcular in the following cells:

January 22, 2015
February 18, 2015

Next we will check to see if the "." holds any data worth saving.
"""

df.dtypes

df['.'].unique()

"""Given that the "." column is empty, we will next delete this column and the old <i>Time</i> column to clean up our data"""

df.shape

df = df.drop(['.'] ,axis=1)
df.head()

df.shape

"""As we can see, from the **df.shape** commands (before/after) that the dataframe has been transformed correctly.

### Graphical Analysis

Sometimes we will use graphical analysis to get a better understanding of the data. In the following subsection we will use the following two graphs to display ***missing values***:

***Left***: Matrix shows the distribution of missing data

***Right***: Bar chart showing the total number of values <i>(anything less than 146,615 means missing data)</i>
"""

figure, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,5))
msno.matrix(df, ax=ax1, sparkline=False, color=(0.1, 0.25, 0.35))
msno.bar(df, ax=ax2, color=(0.25, 0.7, 0.25))
plt.show()

"""-----
<br>

**Left Graph**<br>
The missing data from "Attack Category" is rather sparse, and generally not a concern. 

"Attack Reference" appears to have a lot of missing data that is somewhat evenly spread out.
<br><br>
**Right Graph**<br>
This graph will show us the overall sum of the variables and inversely, the missing values. 

This shows us that we have ~3800 and ~41,0000 missing values in "*Attack Category*" and "*Attack Reference*" respectfully.
<br>
<br>

-----

## Secondary Cleaning

To see more clearly how many data points are missing, the following code will help us see these values in a more aggregated view
"""

df.isnull().sum()

"""We can see above the count of data points with missing values. """

df['Attack subcategory']

df['Attack Reference']

"""<br>

-----

We can see from exploring these two variables that while "Attack subcategory" has valuable data, due to the small amount of data and lack of tacit knowledge, it would be best to remove these entries for a more clean data set.
<br><br>
On the otherhand, we can see that "Attack Reference" appears to add little to no value for our current purposes. So it would most likely be best to remove this dimension (variable/column) completely. We will do this by filling in these missing data values with "Not Registered".
<br><br>

-----
"""

df["Attack subcategory"] = df["Attack subcategory"].fillna("Not Registered")

df.isnull().sum()

df[pd.isnull(df).any(axis=1)].shape

"""After checking to see if we have properly removed the <i>Attack subcategory</i> and checking the shape again, we will see if we have any duplicate line items."""

df[df.duplicated()].shape

print('Dimensions before dropping duplicated rows: ' + str(df.shape))
df = df.drop(df[df.duplicated()].index)
print('Dimensions after dropping duplicated rows: ' + str(df.shape))

"""**The following chart *should* be empty if we did our last process correctly.** """

df[df.duplicated()]

"""-----
<br>
Now that we have reviewed the obvious dimensions with issues, we now need to look at our other dimensions for validity.
<br><br>

The only valid port ranges are between **0** and **65535**.

As we saw from our original investigation, there are values beyond these limits, which means we need to clean up the data
___

<i>***Garbage In, Garbage Out***</i>

The following code will take any entries with invalid port values, and store them into their own dataframe (df)
"""

invalid_SP = (df['Source Port'] < 0) | (df['Source Port'] > 65535)
invalid_DP = (df['Destination Port'] < 0) | (df['Destination Port'] > 65535)
df[invalid_SP | invalid_DP]

"""We will now delete the invalid entries and check the shape...again."""

df = df[~(invalid_SP | invalid_DP)].reset_index(drop=True)

df.shape

"""We will now check some of the data inside the df to make sure it is clean and accurate."""

print('Total number of different protocols:', len(df['Protocol'].unique()))
print('Total number of different Attack categories:', len(df['Attack category'].unique()))
df['Protocol'].unique()

"""As we can see, there are multiple duplications in the list.

<i>TCP, UDP, etc.</i>

The following code will show us the same for the attack categories.
"""

df['Attack category'].unique()

"""As we can see, there are duplicates due to poor spelling. We will remove white space, uppercase the letters and align "<i>BACKDOOR</i>" and "<i>BACKDOORS</i>". We can also see that Analysis has two entries as well."""

df['Protocol'] = df['Protocol'].str.upper().str.strip()
df['Attack category'] = df['Attack category'].str.upper().str.strip()
df['Attack category'] = df['Attack category'].str.strip().replace('BACKDOORS','BACKDOOR')
df['Attack category'] = df['Attack category'].str.strip().replace(' Analytics','Analytics')

"""Check to make sure the operation executed properly. """

print('Total number of different protocols:', len(df['Protocol'].unique()))
print('Total number of different Attack categories:', len(df['Attack category'].unique()))

"""Now to look at <i>Attack Reference</i> for missing values."""

df[pd.isnull(df['Attack Reference'])]

df[pd.isnull(df['Attack Reference'])].shape

"""Now to segment the missing <i>Attack Reference</i> by <i>Attack Category</i>"""

#Missing Values for each category

print(df[pd.isnull(df['Attack Reference'])]['Attack category'].value_counts())

#Total Values for each category

print(df['Attack category'].value_counts())

#Calculates the percentage of values missing
((df[pd.isnull(df['Attack Reference'])]['Attack category'].value_counts()/df['Attack category'].value_counts())*100).dropna().sort_values(ascending=False)

"""## Feature Engineering

We will now add in the TCP-ports.csv values into our working set.

As you will see below, this will give us a better idea of what we are looking at.
"""

tcp_ports = pd.read_csv('https://media.githubusercontent.com/media/BradleyConlin/EDA_Tutorials/main/TCP-ports.csv')
tcp_ports['Service'] = tcp_ports['Service'].str.upper()
tcp_ports.head()

print('Dimensions before merging dataframes: ' ,(df.shape))

newdf = pd.merge(df, tcp_ports[['Port','Service']], left_on='Destination Port', right_on='Port', how='left')
newdf = newdf.rename(columns={'Service':'Destination Port Service'})

print('Dimensions after merging dataframes: ' + str(newdf.shape))

"""Looking at the shape of the newdf or new data frame, we can see that we have the two newly added columns (namely the Destination Port Service.

We will now remove "Port" from the data as this will just be a duplication of data.
"""

newdf = newdf.drop(columns=['Port'])
newdf.head()

"""## Data Cleaning is Complete

Now to move forward into looking into the data more deeply. We will begin by looking at the shape and distribution of the data.

# Step3: Variable *Relationships*

## Data Shape and Distribution
"""

newdf['Attack category'].unique()

# Checking to see the total counts in the Attack Category

newdf['Attack category'].value_counts()

# Checking the pertentage of overall attacks by Attack Category

newdf['Attack category'].value_counts()*100/newdf['Attack category'].value_counts().sum()

"""<h2>What does the data tell us?</h2>
As we can see from the above dataframes, we have unbalanced data. 

- The top 5 categories hold ~95% of the data.

## Graphical Analysis

The following three cells are different ways for us to visualize the data.
"""

plt.figure(figsize=(18,6))
sns.barplot(x=newdf['Attack category'].value_counts().index,y=newdf['Attack category'].value_counts())
plt.xlabel('Attack Category')
plt.ylabel('Count')
plt.title('Number of attacks per Attack caterogy')
plt.grid(True)

pd.DataFrame(newdf['Attack category'].value_counts())[:]

a=pd.DataFrame(newdf['Attack category'].value_counts())[:6]

a.plot(kind='pie', subplots=True, figsize=(12, 12))
plt.title('Top five attacks')
plt.legend(loc='left')
plt.show()

newdf.describe()

"""<h2>Interesting Findings</h2>

- Min & Max are identical
- Mean and 75% do not match

## Hypothesis Testing

$$ H_0: \mu_1=\mu_2$$
$$ H_a: \mu_1\neq\mu_2$$

We can obtain one of two results from the test:

1. If the **$p$-value** is less than our significance level ($p<\alpha$) we reject the null hypothesis $H_0$ and affirm that the observed difference is **statistically significant**.
2. If the **$p$-value** is greater than our significance level ($p>\alpha$) we will have to retain $H_0$ and conclude that the observed difference **is not statistically significant**.

The hypothesis test is conducted using a statistical **$T-test$** which specifies the two Series `df_interest['Source Port']` and `df_interest['Destination Port']`. 

<i>By specifying these two Series, we are automatically referring to a comparative test of the means of both Series:</i>
"""

statistic, pvalue = stats.ttest_ind( newdf['Source Port'], newdf['Destination Port'], equal_var=False)
print('p-value in T-test: ' + str(pvalue))

"""Because the $p$-value is very close to zero, Python approximates this measurement to 0.0. 

Therefore, we can reject the null hypothesis $H_0$ regarding the equality of the means of the source and destination ports. 
___
Meaning that the difference between the two <i>means</i> are significantly different.

### Correlation Coefficiant

We will be using two methods for correlation calculation:

- **<i>Pearson's correlation</i>**: evaluates the linear relationships between two variables. If the value is close to 0, there is a weak or nonexistent linear relationship between the variables.

- **<i>Spearman's correlation</i>**: evaluates the monotonic relationships between two variables. If the value is close to 0, there is a weak or nonexistent monotonic relationship between the variables.

___

<i>Definitions<i>

**Monotonic Relationship**: The variables tend to move in the same relative direction, but not necessarily at a constant rate. 

**Linear Relationship**: The variables move in the same direction at a constant rate.

[Source](https://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/supporting-topics/basics/linear-nonlinear-and-monotonic-relationships/)
"""

newdf.corr(method='pearson')

"""Pearson shows us that the correlation between Source Port and Desitination Port are close to 0, showing signs that there is a weak or nonexistent linear relationship.
 
____

Now to check **Monotonic**
"""

newdf.corr(method='spearman')

"""Using Spearman, we can see a value of 0.68, showing a somewhat strong likelihood that there is some form monotonic relationship.

The following are different ways to visually represent this data.
"""

# df_dummies converts category values into a dummy or indicator value 

df_dummies = pd.get_dummies(newdf, columns=['Attack category'])

plt.figure(figsize=(18,7))
sns.heatmap(df_dummies.corr(method='pearson'), 
            annot=True, vmin=-1.0, vmax=1.0, cmap=sns.color_palette("RdBu_r", 15))
plt.show()

"""<h2>Pearson's Correlation<h2>

As seen with the overall calculation there is not much to see here, except 1 item.

- SHELLCODE & Destination Port
"""

plt.figure(figsize=(18,7))
sns.heatmap(df_dummies.corr(method='spearman'), 
            annot=True, vmin=-1.0, vmax=1.0, cmap=sns.color_palette("RdBu_r", 15))
plt.show()

"""<h2>Spearman's Correlation</h2>

As we saw in the previous calculation, there is a high correlation between Destination and Source Port.

However, there seems to be something interesting with Duration as well, as you can see around the top left corner with .36 and .35.
"""

g = sns.pairplot(newdf)
g.fig.set_size_inches(10,6)
plt.show()

"""The above pairplot shows us that:

- Source Port x Destination Port: Nothing in particular
- Source Port x Duration: Under 20s, high and low ports
- Destination x Duration: Low ports and upper ports short time/lower ports varied with low time concentration

# Source & Destination Port Analysis

To get a better understanding of what is happening with <i>Source Port</i> and <i>Destination Port</i>, we will look to see where the attacks are happening. 


___

We will begin by looking at the IP Addresses being attacked, and see how often they show up in the data.
"""

newdf['Destination IP'].value_counts()

"""The top IP Address has **43,199** attacks on it!

___

We will investigate this IP Address further, and start with time.
"""

plt.figure(figsize=(18,7))
sns.scatterplot(x=newdf[newdf['Destination IP']=='149.171.126.17']['Start time'], y=newdf[newdf['Destination IP']=='149.171.126.17']['Destination Port'])
plt.xlim(left=newdf['Start time'].min()-timedelta(days=1),right=newdf['Start time'].max()+timedelta(days=1))
plt.grid(True)
plt.show()

"""We are seeing some trending here, but we still need to investigate further.

___

We will start with the left side of the scatterplot from above.
"""

plt.figure(figsize=(18,7))
sns.scatterplot(x=newdf[newdf['Destination IP']=='149.171.126.17']['Start time'], y=newdf[newdf['Destination IP']=='149.171.126.17']['Destination Port'])
plt.xlim(left=newdf['Start time'].min(),right=datetime.strptime('15-01-23', '%y-%m-%d'))
plt.grid(True)
plt.show()

"""We can now see that there is a strong concentration of attacks on the lower ports.

___

<h3>Right side Analysis</h3>
"""

plt.figure(figsize=(18,7))
sns.scatterplot(x=newdf[newdf['Destination IP']=='149.171.126.17']['Start time'], y=newdf[newdf['Destination IP']=='149.171.126.17']['Destination Port'])
plt.xlim(left=datetime.strptime('15-02-18', '%y-%m-%d'),right=newdf['Start time'].max())
plt.grid(True)
plt.show()

"""Very similar results to the left side

___

<h3>Ports <150</h3>

Since we have a lot of overlap on the x-axis, I would like to get a better idea of what is happening down where the concentration appears to by the highest.
"""

plt.figure(figsize=(20,7))
sns.scatterplot(x='Start time', y='Destination Port', hue='Attack category', 
                data=newdf[(newdf['Destination IP']=='149.171.126.17')&(newdf['Destination Port']<=150)], 
                s=65)
plt.xlim(left=datetime.strptime('15-02-18 00:00:00', '%y-%m-%d %H:%M:%S'),
         right=datetime.strptime('15-02-18 13:00:00', '%y-%m-%d %H:%M:%S'))
plt.grid(True)
plt.show()

"""We can now see a lot of 

- <i>**Backdoor**</i> attacks happening at the 21 and 25 port, those being the ftp and smtp (mail ports).
- <i>**Port 80**- HTTP</i> has alot of everything, esspecially <I>Exploits</I>

- <i>**Port 110**- POP</i> has alot of everything, esspecially <I>Reconnaissance</I>

# Duration & Destination Port Analysis

To begin we will use a scatterplot to see if there are any insights we can gleen from the data
"""

plt.figure(figsize=(18,7))
sns.scatterplot(x='Destination Port', y='Duration', hue='Attack category', data=newdf[newdf['Destination IP']=='149.171.126.17'])
plt.grid(True)
plt.show()

"""Most of the ports being attacked for long durations appear to be in the very low end of the values.

Below is a violinplot, similar to a boxplot. 

- **Boxplot** shows summary statistics, e.g. mean, median, etc.
- **Violinplot** shows summary statistics & full data distribution
"""

plt.figure(figsize=(18,7))
sns.violinplot(x='Attack category', y='Duration', data=newdf)
plt.grid(True)
plt.show()

"""Most of the Attack Categories appear to be normally distributed, except for SHELLCODE. 

- This violinplot is showing us that SHELLCODE his bimodal

The following code blocks will be used to set up a heatmap graph and pivot table.
"""

def heatmap_graph(df, xlabel, ylabel, title):
    plt.figure(figsize=(18,8))
    ax = sns.heatmap(df)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(title)
    plt.xticks(rotation=90)
    plt.yticks(rotation=0)
    plt.show()

newdf["Start time"][1].hour

df_pivot = newdf.copy()
df_pivot['hour'] = df_pivot.apply(lambda row: '0'*(2-len(str(row['Start time'].hour)))+str(row['Start time'].hour)+':00:00', axis=1)

# Creating a new df for the pivot table

df_pivot[:5]

"""##Pivot Table"""

df_p1 = pd.pivot_table(df_pivot,values='Attack Name', index=['hour'], columns=['Attack category'], aggfunc='count')
df_p1

"""The pivot table shows us some rather random looking numbers but with some form of pattern as well. 

Let us try the heat map

##Heat Maps

The following heat map shows the number of attacks by attack type.
- Black will be <1000 where white is ~10,000 attacks

###Heat Map 1
"""

heatmap_graph(df = df_p1, xlabel = 'Attack category', ylabel = 'Hour', title = 'Heat Map 1: Number of attacks per hour and attack type')

"""**Insights from  Heat Map 1:**
- Analysis, Backdoor, Shellcode & Worms appear to be consistent throughout the day
- The remaining categoriess appear to have higher attacks on the odd hours of the day
- Exploits and DOS have shown to have the strongest patterning

###Heat Map 2
"""

df_p2 = pd.pivot_table(df_pivot, values='Attack Name', index=['hour'], columns=['Destination IP'], aggfunc='count')
heatmap_graph(df = df_p2/df_p2.sum(), xlabel = 'Destination IP', ylabel = 'Hour', title = 'Heat Map 2: Percentage of attacks per IP and hour')

"""**Insights from  Heat Map 2:**
- IPv4 149.171.126.10 had a strong pattern showing of attacks at **5:00AM**
- IPv4 149.171.126.13 had a strong pattern showing of attacks at **1:00AM**
- IPv4 149.171.126.17 showed increased attacks at **7:00AM**, **9:00AM** & **11:00AM**
- IPv4 149.171.126.19 had a strong pattern showing of attacks at **3:00AM**

# Attack Category Analysis

##Heat Map 3

Since it is now clear that specific IPv4 addresses are being targetted, we will look more specifically at this data.
"""

df_p3 = pd.pivot_table(df_pivot, values='Attack Name', index=['Destination IP'], columns=['Attack category'], aggfunc='count')
heatmap_graph(df = df_p3/df_p3.sum(), xlabel = 'Attack category', ylabel = 'Destination IP', title = 'Heat Map 3: Number of attacks per IP and attack type')

"""**Insights from  Heat Map 3:**
- **IPv4 149.171.126.17** had a strongest looking pattern across the 10 main IP addresses targetted 
- **DOS**, **Backdoor** & **Exploits** were the most concentrated attacks
- **Shellcode** has the most consistent attacks over the time period

## Pair-wise T-test

We will now look at the relationship between variables for Source Port and Destination Port, but by each Attack Category. 
- *Similar to before, except we are breaking it down by Attack Category*
"""

for attack in list(newdf['Attack category'].unique()):
    df_attack = newdf[newdf['Attack category'] == attack].copy()
    statistic, pvalue = stats.ttest_ind(df_attack['Source Port'], df_attack['Destination Port'], equal_var=False)
    print('p-value in T-test for ' + attack + ' | ' + str(pvalue))

"""The *ð‘-values* of all but one attack category are very close to 0.0.
- This means that the attacks have been directed to the specific ports *(except Shellcode)*

**Shellcode**: We cannot reject null hypothesiss, therefore there is a defined randomness, which means the source and destination ports have similar averages.

___

To verify this statement, we will make use of a contingency table which allows to relate the count of a certain pair of variables, similar to how we saw the .pivot_table()
"""

df_crosstab = pd.crosstab(newdf['Attack category'], newdf['Destination Port'])
df_crosstab

"""From the contingency table using Attack type and Target port, we can see that the individual counts are not uniform. 
- This helps to affirm our inference that there is potentially some interaction between these two variables
___
We will need to test to see if these variations are actual differences, or outcomes of randomness in the data.

## Chi-square Test

While there are many ways to test the relationship between our variables, we will focus on the **Chi-square test**, as it is one of the more widely used tests, and easy to perform.

The null hypothesis for the Chi-square test is as follows:

$$
H_0:\text {The attack category is independent of the destination port}
$$
"""

chi2, p_value, dof, expected = chi2_contingency(df_crosstab)
print("p-value of Chi-square test for Attack category vs. Destination Port =", p_value)

"""As previously seen in the Hypothesis testing, our value for the Chi-square test is extremely small, and will be shown to us in python as 0.0
- Therefore, the destination port appearss to be somewhat dependant on the Attack category used.

To better visualize this relationship, we will use a scatterplot function to show us the Source Port along the x axis, the Destination Port on the y axis, and use the different attack categories for the colour (or hue).
"""

plt.figure(figsize=(18,7))
sns.scatterplot(x='Source Port',y='Destination Port', hue='Attack category',data=newdf)
plt.show()

"""Emerging Patterns
- Strong concentration of attacks on Destination Ports <10,000 and >50,000
- Strongest concentration at the lowest ports <150
- Shellcode appears to be equally distributed

## Strip Plot

To see this relationship more in depth, we can visualize the the distribution of the Attack Categories and Destination Ports with a strip diagram using the `.stripplot()` function:
"""

# Source ports
plt.figure(figsize=(16,5))
sns.stripplot(x='Attack category',y='Source Port',data=newdf)
plt.show()

"""As we saw with the data previously, there is a pretty even spread of the use of Source Port for all attack types."""

# Destination ports
plt.figure(figsize=(16,5))
sns.stripplot(x='Attack category',y='Destination Port',data=newdf)
plt.show()

"""Similar to Soure Ports, Destination Ports are showing us the same results ass the data. 
- Recon, DOS, Gen, Fuz, Worms, Ana all have a strong trend to target low number ports
- Exploits are relatively spread out, but with concentrations at the upper and lower bounds
- Shellcode is relatively uniformly distributed

## IPv4 Analysis

We will now look at the Attack Categories by
"""

# Find unique Attacker IPv4

list(newdf['Source IP'].unique())

"""### Attacker IPv4

Below will split out the four unique IP Addresses we have for attackers, and compare the distribution of Attack Categories to the Destination Ports.
"""

ips = list(newdf['Source IP'].unique())
f, axes = plt.subplots(6,2)
f.set_figheight(40)
f.set_figwidth(20)

labels = list(newdf['Attack category'].unique())
for i, ip in enumerate(ips):
    sns.stripplot(x='Attack category',y='Destination Port',data=newdf[newdf['Source IP'] == ip], order=labels, ax=axes[int(i/2)][i%2])
    axes[int(i/2)][i%2].set_xlabel('Attack category')
    axes[int(i/2)][i%2].set_ylabel('Destination Port')
    axes[int(i/2)][i%2].set_title('Destination Port distribution - Attacker IPv4 Address: ' + ip)
    axes[int(i/2)][i%2].set_xticklabels(labels,rotation=90)
plt.tight_layout()
plt.show()

"""While there are obviously going to be some variance between the four graphs, we can clearly see similarity among the four different IPv4 values explored.

### Victim IPv4

We will complete the same exercise as above, but for the victim IP's.
"""

list(newdf['Destination IP'].unique())

ips = list(newdf['Destination IP'].unique())
f, axes = plt.subplots(5, 2)
f.set_figheight(20)
f.set_figwidth(15)

labels = list(newdf['Attack category'].unique())

for i, ip in enumerate(ips):
    sns.stripplot(x='Attack category',y='Destination Port',data=newdf[newdf['Destination IP'] == ip], order=labels, ax=axes[int(i/2)][i%2])
    axes[int(i/2)][i%2].set_xlabel('Attack category')
    axes[int(i/2)][i%2].set_ylabel('Destination Port')
    axes[int(i/2)][i%2].set_title('Destination Port distribution - Target IPv4 Address: ' + ip)
    axes[int(i/2)][i%2].set_xticklabels(labels,rotation=90)
plt.tight_layout()
plt.show()

"""We can see that there is some basic variance between the IPv4 addresses and how they were attacked, namely DOS, but they are all relatively aligned with what our previous analysis has shown.

# Conclusion

1. Most targeted Destination IP Address

2. Most Ports attacked

3. Most Frequently/common type of Attack

4. Different time of the day , (odd , hours, day or night)

5. Find any patterns?
"""